{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a set of tweets from a hashtag or a user, plot the cloud of its embeddings.\n",
    "# An embedding of a tweet is a probability over latent topics.\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import condor\n",
    "from condor.config import PATHS\n",
    "from condor.utils.utils_json import load_jsonl\n",
    "import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "keyword=\"hola\"\n",
    "filename = PATHS['tracked'] + keyword + \".jsonl\"\n",
    "data = load_jsonl(filename)\n",
    "data = data[0:10]\n",
    "tweets = [d['text'] for d in data]\n",
    "pprint.pprint(tweets)\n",
    "embeddings_LDA(tweets)\n",
    "\n",
    "n_features = 1000\n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, \n",
    "                                min_df=0.2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=None)                         \n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                            learning_method='online',\n",
    "                            learning_offset=50.,\n",
    "                            random_state=0)\n",
    "lda.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words=100)\n",
    "params = lda.get_params()\n",
    "print(params)\n",
    "\n",
    "# Show topic distribution over words\n",
    "# https://stackoverflow.com/questions/44208501/getting-topic-word-distribution-from-lda-in-scikit-learn\n",
    "topic_embeddings = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "print(topic_embeddings)\n",
    "# TODO project with t-SNE\n",
    "# t SNE ok for user embeddings.\n",
    "# Will be harder for product embeddings, too many dimensions.\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(topic_embeddings)\n",
    "\n",
    "N = 10000\n",
    "df = pd.DataFrame(tsne_results)\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "\n",
    "df_subset = df.loc[rndperm[:N],:].copy()\n",
    "df_subset['tsne-one'] = tsne_results[:,0]\n",
    "df_subset['tsne-two'] = tsne_results[:,1]\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-one\", y=\"tsne-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ]
}